{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHw14bCDYtslY8bbPMx/Tl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calicartels/Explainable-Techniques-LIME/blob/main/Explainable_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable Techniques : LIME"
      ],
      "metadata": {
        "id": "8mTkerH-iG5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning the repo:"
      ],
      "metadata": {
        "id": "7XFSFwIilRBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/calicartels/Explainable-Techniques-LIME.git"
      ],
      "metadata": {
        "id": "qtQp7KUslULF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will install the necessary libraries. In addition to numpy and pandas, we will use the following libraries:\n",
        "\n",
        "\n",
        "*   scikit-learn (Model, ICE plots)\n",
        "*   lime (LIME)\n"
      ],
      "metadata": {
        "id": "YcSkSBv8arP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MtlKvrf1YosD"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.25.2 pandas==2.0.3 matplotlib==3.7.1 scikit-learn==1.2.2 lime==0.2.0.1 anchor-exp==0.0.2.0 shap==0.45.1 xgboost==1.7.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Models\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        "from keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "\n",
        "# XAI\n",
        "import lime\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "np.random.seed(1)"
      ],
      "metadata": {
        "id": "g0ge8B4pZDFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Enabling GPU usage\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name:\n",
        "    print(f\"Using GPU: {device_name}\")\n",
        "else:\n",
        "    print(\"No GPU found. Using CPU.\")\n",
        "\n",
        "# Loading pre-trained InceptionV3 model + weights\n",
        "model = InceptionV3(weights='imagenet')\n",
        "\n",
        "# Loading an image (using a sample class here, dalmatian)\n",
        "img_path = \"/content/Explainable-Techniques-LIME/dalmation.jpeg\"\n",
        "img = image.load_img(img_path, target_size=(299, 299))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array = preprocess_input(img_array)\n",
        "\n",
        "# Predicting using InceptionV3\n",
        "preds = model.predict(img_array)\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
        "\n",
        "# LIME Image explanation\n",
        "explainer = lime_image.LimeImageExplainer()\n"
      ],
      "metadata": {
        "id": "5k5YUV5cZpUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explaining prediction for the image\n",
        "explanation = explainer.explain_instance(img_array[0], model.predict, top_labels=5, hide_color=0, num_samples=1000)\n",
        "\n",
        "# Visualizing the explanation\n",
        "temp, mask = explanation.get_image_and_mask(\n",
        "    explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False\n",
        ")\n",
        "\n",
        "\n",
        "plt.imshow(mark_boundaries(img_array[0] / 2 + 0.5, mask))  # Rescaling to original image range\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zZUjU5bzb_9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing superpixels\n",
        "temp, mask = explanation.get_image_and_mask(\n",
        "    explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False\n",
        ")\n",
        "\n",
        "# Displaying original image with only the important superpixels highlighted\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Original image with LIME explanation\n",
        "ax[0].imshow(mark_boundaries(img_array[0] / 2 + 0.5, mask))\n",
        "ax[0].set_title(\"LIME Explanation\")\n",
        "\n",
        "# Superpixels alone (highlighted with color)\n",
        "ax[1].imshow(mask, cmap='gray')\n",
        "ax[1].set_title(\"Highlighted Superpixels\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aY7qx_mIg9j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\t1.\tLeft Side (LIME Explanation): The original image with important regions (superpixels) overlaid.\n",
        "\t2.\tRight Side (Superpixels Alone): A grayscale image where only the selected superpixels are highlighted, making it easier to understand which parts of the image the model focused on.\n",
        "\n",
        "This visualization provides another layer of insight by isolating the important regions in the image that LIME considers influential for the prediction.\n"
      ],
      "metadata": {
        "id": "ByRvAc1emJad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why LIME?\n",
        "\n",
        "LIME (Local Interpretable Model-agnostic Explanations) is a great choice for image models like InceptionV3 because:\n",
        "\n",
        "\t•\tIt explains individual predictions by highlighting regions of the image that were most influential in the model’s decision.\n",
        "\t•\tIt works by perturbing the input image and observing how changes impact the model’s predictions, offering visual insights into the black-box model.\n",
        "\n",
        "Strengths:\n",
        "\n",
        "\t•\tLocal interpretability: LIME helps understand which parts of the image influenced the model’s decision for that specific image.\n",
        "\t•\tModel-agnostic: It works with any model, including deep neural networks like Inception.\n",
        "\t•\tVisual explanations: It highlights important image regions, providing clear visual feedback.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "\t•\tApproximate explanation: The explanation is based on a linear model fitted locally to perturbations of the image, so it might not capture complex decision boundaries accurately.\n",
        "\t•\tPerturbation-based: LIME’s reliance on random perturbations can make it computationally expensive, especially with high-resolution images.\n",
        "\n",
        "Potential Improvements:\n",
        "\n",
        "\t•\tBetter segmentations: Improving the way LIME segments images could lead to more accurate explanations.\n",
        "\t•\tHybrid methods: Combining LIME with methods like SHAP could yield more theoretically sound explanations.\n"
      ],
      "metadata": {
        "id": "XRmw3ADGhO_r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FGMqC0qGiuau"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}